
# 环境
```bash
conda create --name d2l python=3.9 -y
conda activate d2l
pip install torch==1.12.0
pip install torchvision==0.13.0
pip install d2l==0.17.6
```

# 预备知识
## 线性代数
### Hadamard积
对应元素乘积
```
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = a.clone()
c = a * b
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```
### 向量-点积
对应元素乘积的和
```python 
torch.dot(x, y) == torch.sum(x * y)
```
### 矩阵-向量积torch.mv

```python
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = torch.arange(4, dtype=torch.float32)

tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])

tensor([ 0.,  1.,  2.,  3.])

torch.mv(a, b)
tensor([ 14.,  38.,  62.,  86., 110.])

```

### 矩阵乘法torch.mm
```python
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = torch.arange(20, dtype=torch.float32).reshape(4, 5)
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])

tensor([[ 0.,  1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.,  9.],
        [10., 11., 12., 13., 14.],
        [15., 16., 17., 18., 19.]])
torch.mm(a,b)
```

### 范数torch.norm
```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

计算向量夹角
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

cos_angle = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
angle = np.arccos(cos_angle)

余弦相似度
import torch.nn.functional as F

vec1 = torch.FloatTensor([1, 2, 3, 4])
vec2 = torch.FloatTensor([5, 6, 7, 8])

cos_sim = F.cosine_similarity(vec1, vec2, dim=0)
```

## 微积分
x（向量）是叶子节点，z（向量）是中间节点，y（标量）是输出节点
Tensor的属性，requires_grad， grad微分值，grad_fn微分函数
- 当叶子节点的requires_grad为True时，信息流过该节点，所有中间节点的requires_grad都变为true
- 只要在输出节点调用backward(), Pytorch就会自动更新叶子节点的微分值，存储在叶子节点的grad属性里
- 只有叶子节点的grad属性能被更新
### 梯度-前向传播
```python
x = torch.ones(2)
x.requires_grad=True
z= 4*x
tensor([4., 4.], grad_fn=<MulBackward0>) # tensor是一个矢量
y=z.norm()
tensor(5.6569, grad_fn=<LinalgVectorNormBackward0>) # tensor是一个标量
```
### 梯度-反向传播
tensor做计算都会产生计算图，用于反向传播，计算梯度
```python
x.backward() # 报错grad can be implicitly created only for scalar outputs， 只能作用于标量
y.backward()
x.grad
tensor([2.8284, 2.8284])
z.grad
y.grad
```

计算函数y=x*x的梯度
```python
a = torch.arange(-3,4, dtype=torch.float32)
a.requires_grad = True
b = a * a
c = b.norm()
c.backward()

print(a.grad)
tensor([-3.8571, -1.1429, -0.1429,  0.0000,  0.1429,  1.1429,  3.8571]) # 在0点的梯度为0， 2边梯度逐渐变大
```
**backward() 只能作用于标量，如果是矢量，需要增加一个参数gradient, gradientde的形状跟输出节点的形状保持一致，而且元素均值为1**