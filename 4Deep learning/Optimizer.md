# Adam
- 自适应调整学习率：Adam 优化器可以根据历史梯度信息来自适应地调节学习率，使得在训练初期使用较大的学习率，能够快速收敛，在训练后期使用较小的学习率，能够更加准确地找到损失函数的最小值。
- 调整动量：Adam 优化器能够调整动量参数，以平衡上一次梯度和当前梯度对参数更新的影响，从而避免过早陷入局部极小值。
- 归一化处理：Adam 优化器对参数的更新进行了归一化处理，使得每个参数的更新都有一个相似的量级，从而提高训练效果。
- 防止过拟合：Adam 优化器结合了L2正则化的思想，在更新时对参数进行正则化，从而防止神经网络过度拟合训练数据。

# AdamW
权重衰减（Weight Decay）是一种正则化技术，用于防止深度学习模型的过拟合。

# LAMB
LAMB 优化器是 2019 年出现的一匹新秀，它将bert模型的预训练时间从3天压缩到了76分钟！ LAMB 出现的目的是加速预训练进程，这个优化器也成为 NLP 社区为泛机器学习领域做出的一大贡献。在使用 Adam 和 AdamW 等优化器时，一大问题在于 batch size 存在一定的隐式上限，一旦突破这个上限，梯度更新极端的取值会导致自适应学习率调整后极为困难的收敛，从而无法享受增加的 batch size 带来的提速增益。LAMB 优化器的作用便在于使模型在进行大批量数据训练时，能够维持梯度更新的精度。具体来说，LAMB 优化器支持自适应元素级更新（adaptive element-wise updating）和准确的逐层修正（layer-wise correction）
