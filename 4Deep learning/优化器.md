# Adam
- 自适应调整学习率：Adam 优化器可以根据历史梯度信息来自适应地调节学习率，使得在训练初期使用较大的学习率，能够快速收敛，在训练后期使用较小的学习率，能够更加准确地找到损失函数的最小值。
- 调整动量：Adam 优化器能够调整动量参数，以平衡上一次梯度和当前梯度对参数更新的影响，从而避免过早陷入局部极小值。
- 归一化处理：Adam 优化器对参数的更新进行了归一化处理，使得每个参数的更新都有一个相似的量级，从而提高训练效果。
- 防止过拟合：Adam 优化器结合了L2正则化的思想，在更新时对参数进行正则化，从而防止神经网络过度拟合训练数据。
