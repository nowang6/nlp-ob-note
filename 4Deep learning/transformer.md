

![[Pasted image 20231226213351.png]]


![[Pasted image 20231226213542.png]]


# Attention机制的优化
稀疏注意力：限制查询Q-键K对
Flash Attention: 显存和缓存优化
多查询注意力：不同头共享键K-值V组合

滑窗注意力: 用基于滑动窗口的注意力替换完整注意力 (平方级计算成本)，其中每个词元最多可以关注上一层的 4096 个词元 (线性计算成本)。这样，多层以后，Mistral 7B 的实际关注词元数会叠加，因此更高层的注意力实际关注的总历史词元数会超过 4096。

分组查询注意力: Llama 2 也使用了该技术，其通过缓存先前解码的词元的键向量和值向量来优化推理过程 (减少处理时间)。



# Subword算法

## BPE
从基本词汇表中选择组合在一起出现频次最高的两个符号中，并将其合成一个新的符号，加入基础词汇表，直到达到提前设定好的词汇量为止（词汇表大小是超参数）
