

![[Pasted image 20231226213351.png]]


![[Pasted image 20231226213542.png]]


# Attention机制的优化
稀疏注意力：限制查询Q-键K对
Flash Attention: 显存和缓存优化
多查询注意力：不同头共享键K-值V组合

滑窗注意力: 用基于滑动窗口的注意力替换完整注意力 (平方级计算成本)，其中每个词元最多可以关注上一层的 4096 个词元 (线性计算成本)。这样，多层以后，Mistral 7B 的实际关注词元数会叠加，因此更高层的注意力实际关注的总历史词元数会超过 4096。

分组查询注意力: Llama 2 也使用了该技术，其通过缓存先前解码的词元的键向量和值向量来优化推理过程 (减少处理时间)。



# Subword算法

## BPE
从基本词汇表中选择组合在一起出现频次最高的两个符号中，并将其合成一个新的符号，加入基础词汇表，直到达到提前设定好的词汇量为止（词汇表大小是超参数）

# PE Position Encode位置编码
## 三角函数
## 旋转位置编码



# Transfomers
## Transfomers的意义
Transforme它于2017年由Google的论文《Attention is All You Need》中提出。主要有意向创新点
1. 自注意机制：它允许模型在处理序列数据时直接计算序列中任意两个位置之间的依赖关系，无需像传统RNN和CNN那样逐步传递信息或依赖固定大小的窗口。
2. 多头注意力：Transformer可以从不同的表示子空间捕获丰富的信息，增强了模型的表示能力。
3. 位置编码：
4. 并行处理能力：
5. 灵活的架构：可以堆叠。

## Transfomers的影响和局限
1. 预训练模型的兴起。
2. 大模型。
3. CV领域

## 问题：
1. 计算资源要求高
2. 长序列处理能力。


## Transfomer结构
Transformer模型的结构是基于编码器-解码器（Encoder-Decoder）架构的，旨在处理序列到序列（seq2seq）的任务，如机器翻译。

### 编码器（Encoder）
编码器由N个相同的层堆叠而成（典型的N值为6）。每一层包含两个主要的子层：
- 自注意力层（Self-Attention Layer）：这个层允许每个位置的输入序列在编码时考虑到序列中的其他所有位置，从而捕获了序列内的全局依赖关系。
- 前馈神经网络（Feed-Forward Neural Network）：对自注意力层的输出进行处理的一个全连接的前馈网络。每个位置都会使用相同的前馈网络，但是它们不共享参数。
每个子层之前都有残差连接（Residual Connection），并且每个子层的输出都会进行层归一化（Layer Normalization）。


## 解码器（Decoder）
- 遮蔽自注意力层（Masked Self-Attention Layer）：与编码器中的自注意力层类似，但添加了遮蔽（Masking）以确保位置只能依赖于之前的位置，这对于预防信息泄露至解码器尚未生成的部分至关重要。
- 编码器-解码器注意力层（Encoder-Decoder Attention Layer）：这个层使解码器能够关注（Attention）到编码器的输出。具体来说，解码器的查询（Q）来自于前一个解码器层的输出，而键（K）和值（V）来自于编码器的输出。
- 前馈神经网络：与编码器中的前馈网络相同，但参数是独立的。

解码器的每个子层同样使用了残差连接和层归一化
